#
# Copyright 2021-2023 Aklivity Inc
#
# Licensed under the Aklivity Community License (the "License"); you may not use
# this file except in compliance with the License.  You may obtain a copy of the
# License at
#
#   https://www.aklivity.io/aklivity-community-license/
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

accept "zilla://streams/kafka0"
    option zilla:window 8192
    option zilla:transmission "duplex"

accepted

read zilla:begin.ext ${kafka:beginEx()
                            .typeId(zilla:id("kafka"))
                            .merged()
                                .capabilities("PRODUCE_ONLY")
                                .topic("mqtt-messages")
                                .groupId("client-publish")
                                .partition(-1, -2)
                                .ackMode("IN_SYNC_REPLICAS")
                                .producerId(12345) # generated from clientId, this tells kafka that we're using idempotent producer
                                .build()
                            .build()}

# This begin triggers InitProducerId with transactional_id = null, producerId = 12345
# If this is the first time this producerId is used, do we get back UNKNOWN_PRODUCER_ID error?
# If yes, we'll need to call InitProducerId with transactional_id = null again, this time not specifying the producerId

write zilla:begin.ext ${kafka:beginEx()
                            .typeId(zilla:id("kafka"))
                            .merged()
                                .capabilities("PRODUCE_ONLY")
                                .topic("mqtt-messages")
                                .partition(0, 0, 1, 1) # offset = next sequence number
                                .producerId(67890, 1) #producerId, epoch -> save this in memory along with next sequence number
                                .build()
                            .build()}

connected

read zilla:data.ext ${kafka:matchDataEx()
                           .typeId(zilla:id("kafka"))
                           .merged()
                            .produce()
                               .deferred(0)
                               .partition(-1, -1)
                               .key("sensor/one")
                               .header("zilla:filter", "sensor")
                               .header("zilla:filter", "one")
                               .header("zilla:local", "client")
                               .header("zilla:qos", "2")
                               .build()
                           .build()}
read "message"

# I guess we don't need to specify producerId + epoch + sequence in the data frame?
# Along with the data frame we also send an offset commit with metadata

read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3,
                                        mqtt:metadata()
                                            .metadata(67890, 1, 1) # producerId, producerEpoch, packetId
                                            .build())
                                  .correlationId(1)
                                  .build()
                              .build()}

write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3)
                                  .correlationId(1)
                                  .build()
                              .build()}
# This is what triggers PUBREC delivery to the client

# receiving PUBREL in the MqttServerFactory triggers this:
read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3, "")
                                  .build()
                              .build()}

# This is what triggers PUBCOMP delivery to the client
write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3)
                                  .build()
                              .build()}
