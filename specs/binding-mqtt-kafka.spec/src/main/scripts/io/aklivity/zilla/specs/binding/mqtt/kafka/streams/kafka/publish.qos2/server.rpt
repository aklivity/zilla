#
# Copyright 2021-2023 Aklivity Inc
#
# Licensed under the Aklivity Community License (the "License"); you may not use
# this file except in compliance with the License.  You may obtain a copy of the
# License at
#
#   https://www.aklivity.io/aklivity-community-license/
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OF ANY KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations under the License.
#

accept "zilla://streams/kafka0"
    option zilla:window 8192
    option zilla:transmission "duplex"

accepted

read zilla:begin.ext ${kafka:beginEx()
                            .typeId(zilla:id("kafka"))
                            .merged()
                                .capabilities("PRODUCE_ONLY")
                                .topic("mqtt-messages")
                                .groupId("client-publish")
                                .partition(-1, -2)
                                .ackMode("IN_SYNC_REPLICAS")
                                .producerId(0)
                                .build()
                            .build()}

write zilla:begin.ext ${kafka:beginEx()
                            .typeId(zilla:id("kafka"))
                            .merged()
                                .capabilities("PRODUCE_ONLY")
                                .topic("mqtt-messages")
                                .partition(0, 0, 1, 1) # offset = next sequence number, no metadata -> trigger init producer in kafka
                                .build()
                            .build()}

connected

# This should trigger the init producer call to Kafka
read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .produce()
                                  .build()
                              .build()}

write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .produce()
                                  .producerId(12345)
                                  .epoch(0)
                                  .sequence(0)
                                  .build()
                              .build()}

# We need to save the producerId, epoch immediately so if connection drops,
# we don't create another producer for this client. We can use "dummy" offset commit for this
read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 1,
                                        mqtt:metadata()
                                            .metadata(12345, 0) # producerId, producerEpoch
                                            .build())
                                  .build()
                              .build()}

write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 1,
                                        mqtt:metadata()
                                            .metadata(12345, 0) # producerId, producerEpoch
                                            .build())
                                  .build()
                              .build()}
# Only after this we can send window to the mqtt stream
# (if first produce succeeds but offset commit does not, we can still recover the same producer from now on)

read zilla:data.ext ${kafka:matchDataEx()
                           .typeId(zilla:id("kafka"))
                           .merged()
                            .produce()
                               .deferred(0)
                               .partition(-1, -1)
                               .key("sensor/one")
                               .producerId(12345)
                               .epoch(0)
                               .sequence(1)
                               .header("zilla:filter", "sensor")
                               .header("zilla:filter", "one")
                               .header("zilla:local", "client")
                               .header("zilla:qos", "2")
                               .build()
                           .build()}
read "message"

# After kafka binding acked the data frame we also send an offset commit with metadata
# (store <sequence, packetId> in a map, and do this for every entry in the map where sequence <= ack

read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 2,
                                        mqtt:metadata()
                                            .metadata(12345, 0, 1) # producerId, producerEpoch, packetId
                                            .build())
                                  .correlationId(1)
                                  .build()
                              .build()}

write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 2)
                                  .correlationId(1)
                                  .build()
                              .build()}
# This is what triggers PUBREC delivery to the client

# receiving PUBREL in the MqttServerFactory triggers this:
read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 2, "")
                                  .correlationId(1)
                                  .build()
                              .build()}

# This is what triggers PUBCOMP delivery to the client, also we have to increase the sequence number
write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 2)
                                  .correlationId(1)
                                  .build()
                              .build()}



read zilla:data.ext ${kafka:matchDataEx()
                           .typeId(zilla:id("kafka"))
                           .merged()
                            .produce()
                               .deferred(0)
                               .partition(-1, -1)
                               .key("sensor/one")
                               .producerId(12345)
                               .epoch(0)
                               .sequence(2)
                               .header("zilla:filter", "sensor")
                               .header("zilla:filter", "one")
                               .header("zilla:local", "client")
                               .header("zilla:qos", "2")
                               .build()
                           .build()}
read "message2"

# After kafka binding acked the data frame we also send an offset commit with metadata
# (store <sequence, packetId> in a map, and do this for every entry in the map where sequence <= ack

read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3,
                                        mqtt:metadata()
                                            .metadata(12345, 0, 1) # producerId, producerEpoch, packetId
                                            .build())
                                  .correlationId(1)
                                  .build()
                              .build()}

write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3)
                                  .correlationId(1)
                                  .build()
                              .build()}
# This is what triggers PUBREC delivery to the client

# receiving PUBREL in the MqttServerFactory triggers this
read advised zilla:flush ${kafka:matchFlushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3, "")
                                  .correlationId(1)
                                  .build()
                              .build()}

# This is what triggers PUBCOMP delivery to the client, also we have to increase the sequence number
write advise zilla:flush ${kafka:flushEx()
                              .typeId(zilla:id("kafka"))
                              .merged()
                                .consumer()
                                  .progress(0, 3)
                                  .correlationId(1)
                                  .build()
                              .build()}
